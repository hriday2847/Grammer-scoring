{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pickle\n\n# 2. Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 3. Load Data\ntrain_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv\")\n\ntrain_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/\"\ntest_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test/\"\n\n# 4. Load Wav2Vec2 Model\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\nmodel.eval()\n\n# 5. Dataset\nclass GrammarAudioDataset(Dataset):\n    def __init__(self, df, path, processor, is_train=True):\n        self.df = df\n        self.path = path\n        self.processor = processor\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file = self.df.iloc[idx][\"filename\"]\n        file_path = os.path.join(self.path, file)\n        if not os.path.exists(file_path):\n            print(f\"Missing file: {file_path}\")\n            return None\n\n        waveform, sr = torchaudio.load(file_path)\n\n# Ensure mono audio\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)  # Convert to mono\n        waveform = waveform.squeeze().numpy()\n        resampled = librosa.resample(waveform, orig_sr=sr, target_sr=16000)\n\n\n        return {\n            \"input\": torch.tensor(resampled, dtype=torch.float),\n            \"label\": float(self.df.iloc[idx][\"label\"]) if self.is_train else None\n        }\n\n# 6. Collate Function\ndef custom_collate_fn(batch):\n    batch = [b for b in batch if b is not None]\n    if len(batch) == 0:\n        return None\n\n    inputs = [item[\"input\"] for item in batch]\n    padded_inputs = pad_sequence(inputs, batch_first=True)\n\n    if batch[0][\"label\"] is not None:\n        labels = [item[\"label\"] for item in batch]\n        return padded_inputs, torch.tensor(labels, dtype=torch.float)\n    else:\n        return padded_inputs\n\n# 7. Attention-based Pooling (can replace with mean if needed)\ndef attn_pooling(hidden_states):\n    # Simple mean pooling\n    return hidden_states.mean(dim=1)\n\n# 8. Feature Extraction\ndef extract_features(dataloader):\n    features, labels = [], []\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            if batch is None:\n                continue\n            if isinstance(batch, tuple):\n                x, y = batch\n                x = x.to(device)\n                labels.extend(y.cpu().numpy())\n            else:\n                x = batch.to(device)\n\n            hidden_states = model(x).last_hidden_state\n            pooled = attn_pooling(hidden_states).cpu().numpy()\n            features.extend(pooled)\n\n    return np.array(features), np.array(labels) if labels else None\n\n# 9. Prepare Train Data\ntrain_dataset = GrammarAudioDataset(train_df, train_path, processor, is_train=True)\ntrain_loader = DataLoader(train_dataset, batch_size=4, collate_fn=custom_collate_fn)\n\n# Use pickle to cache features\nif os.path.exists(\"train_features.pkl\"):\n    with open(\"train_features.pkl\", \"rb\") as f:\n        X_train, y_train = pickle.load(f)\nelse:\n    X_train, y_train = extract_features(train_loader)\n    with open(\"train_features.pkl\", \"wb\") as f:\n        pickle.dump((X_train, y_train), f)\n\n# 10. Train Regressor\nregressor = Ridge(alpha=1.0)\nregressor.fit(X_train, y_train)\nscores = cross_val_score(regressor, X_train, y_train, scoring='r2', cv=5)\nprint(\"CV R^2 Scores:\", scores)\n\n# 11. Evaluation\ny_pred = regressor.predict(X_train)\npearson_corr = pearsonr(y_train, y_pred)[0]\nrmse = np.sqrt(mean_squared_error(y_train, y_pred))\nprint(f\"Train Pearson Correlation: {pearson_corr:.4f}\")\nprint(f\"Train RMSE: {rmse:.4f}\")\n\n# 12. Visualize Predictions\nplt.figure(figsize=(8,5))\nsns.scatterplot(x=y_train, y=y_pred)\nplt.xlabel(\"Actual Grammar Score\")\nplt.ylabel(\"Predicted Score\")\nplt.title(\"Predicted vs Actual Grammar Scores\")\nplt.grid(True)\nplt.show()\n\n# 13. Inference on Test Set\ntest_dataset = GrammarAudioDataset(test_df, test_path, processor, is_train=False)\ntest_loader = DataLoader(test_dataset, batch_size=4, collate_fn=custom_collate_fn)\n\nX_test, _ = extract_features(test_loader)\ntest_preds = regressor.predict(X_test)\n\nsample_submission[\"label\"] = test_preds\nsample_submission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission saved as submission.csv\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_submission = pd.read_csv('submission.csv')\ndf_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install openai-whisper\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport whisper\nimport xgboost as xgb\nfrom scipy.stats import pearsonr, skew\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 2. Paths\ntrain_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv\")\n\ntrain_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/\"\ntest_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test/\"\n\n# 3. Whisper Embeddings + MFCC Extraction\ndef extract_features(df, audio_path, model):\n    features = []\n    for fname in tqdm(df['filename']):\n        path = os.path.join(audio_path, fname)\n        y, sr = librosa.load(path, sr=16000)\n\n        # MFCC stats\n        mfcc = librosa.feature.mfcc(y=y, sr=16000, n_mfcc=13)\n        mfcc_mean = np.mean(mfcc, axis=1)\n        mfcc_std = np.std(mfcc, axis=1)\n        mfcc_skew = skew(mfcc, axis=1)\n\n        # Whisper embeddings (we use the encoder output from tiny or base model)\n        audio = whisper.load_audio(path)\n        audio = whisper.pad_or_trim(audio)\n        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n        with torch.no_grad():\n            embedding = model.encoder(mel.unsqueeze(0))  # shape: (1, frames, dim)\n            whisper_feat = embedding[0].mean(dim=0).cpu().numpy()\n\n        # Combine\n        feature_vector = np.hstack([mfcc_mean, mfcc_std, mfcc_skew, whisper_feat])\n        features.append(feature_vector)\n\n    return np.array(features)\n\n# 4. Load Whisper Model\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nwhisper_model = whisper.load_model(\"base\", device=device)  # You can try \"small\" or \"medium\"\n\n# 5. Extract Features\nX_train = extract_features(train_df, train_path, whisper_model)\ny_train = train_df[\"label\"].values\nX_test = extract_features(test_df, test_path, whisper_model)\n\n# 6. Handle NaN/Inf\nfor arr in [X_train, X_test]:\n    arr[np.isnan(arr)] = 0\n    arr[np.isinf(arr)] = 0\n\n# 7. XGBoost Regressor\nxgb_model = xgb.XGBRegressor(\n    n_estimators=200,\n    max_depth=6,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1\n)\nxgb_model.fit(X_train, y_train)\n\n# 8. Evaluation\ny_pred_train = xgb_model.predict(X_train)\npearson_corr = pearsonr(y_train, y_pred_train)[0]\nrmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\nprint(f\"✅ Pearson Correlation (Train): {pearson_corr:.4f}\")\nprint(f\"✅ RMSE (Train): {rmse:.4f}\")\n\n# 9. Visualizations\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=y_train, y=y_pred_train)\nplt.xlabel(\"Actual Grammar Score\")\nplt.ylabel(\"Predicted Score\")\nplt.title(\"Predicted vs Actual (Train)\")\nplt.grid(True)\nplt.show()\n\nresiduals = y_train - y_pred_train\nplt.figure(figsize=(8, 4))\nsns.histplot(residuals, kde=True)\nplt.title(\"Residuals Distribution\")\nplt.xlabel(\"Residuals\")\nplt.grid(True)\nplt.show()\n\n# 10. Test Prediction\ntest_preds = xgb_model.predict(X_test)\nsample_submission[\"label\"] = test_preds\nsample_submission.to_csv(\"submission_whisper_xgb.csv\", index=False)\nprint(\"📁 Submission saved as submission_whisper_xgb.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}