{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_submission = pd.read_csv('submission.csv')\ndf_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install openai-whisper\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport whisper\nimport xgboost as xgb\nfrom scipy.stats import pearsonr, skew\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 2. Paths\ntrain_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv\")\n\ntrain_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/\"\ntest_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test/\"\n\n# 3. Whisper Embeddings + MFCC Extraction\ndef extract_features(df, audio_path, model):\n    features = []\n    for fname in tqdm(df['filename']):\n        path = os.path.join(audio_path, fname)\n        y, sr = librosa.load(path, sr=16000)\n\n        # MFCC stats\n        mfcc = librosa.feature.mfcc(y=y, sr=16000, n_mfcc=13)\n        mfcc_mean = np.mean(mfcc, axis=1)\n        mfcc_std = np.std(mfcc, axis=1)\n        mfcc_skew = skew(mfcc, axis=1)\n\n        # Whisper embeddings (we use the encoder output from tiny or base model)\n        audio = whisper.load_audio(path)\n        audio = whisper.pad_or_trim(audio)\n        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n        with torch.no_grad():\n            embedding = model.encoder(mel.unsqueeze(0))  # shape: (1, frames, dim)\n            whisper_feat = embedding[0].mean(dim=0).cpu().numpy()\n\n        # Combine\n        feature_vector = np.hstack([mfcc_mean, mfcc_std, mfcc_skew, whisper_feat])\n        features.append(feature_vector)\n\n    return np.array(features)\n\n# 4. Load Whisper Model\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nwhisper_model = whisper.load_model(\"base\", device=device)  # You can try \"small\" or \"medium\"\n\n# 5. Extract Features\nX_train = extract_features(train_df, train_path, whisper_model)\ny_train = train_df[\"label\"].values\nX_test = extract_features(test_df, test_path, whisper_model)\n\n# 6. Handle NaN/Inf\nfor arr in [X_train, X_test]:\n    arr[np.isnan(arr)] = 0\n    arr[np.isinf(arr)] = 0\n\n# 7. XGBoost Regressor\nxgb_model = xgb.XGBRegressor(\n    n_estimators=200,\n    max_depth=6,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1\n)\nxgb_model.fit(X_train, y_train)\n\n# 8. Evaluation\ny_pred_train = xgb_model.predict(X_train)\npearson_corr = pearsonr(y_train, y_pred_train)[0]\nrmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\nprint(f\"‚úÖ Pearson Correlation (Train): {pearson_corr:.4f}\")\nprint(f\"‚úÖ RMSE (Train): {rmse:.4f}\")\n\n# 9. Visualizations\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=y_train, y=y_pred_train)\nplt.xlabel(\"Actual Grammar Score\")\nplt.ylabel(\"Predicted Score\")\nplt.title(\"Predicted vs Actual (Train)\")\nplt.grid(True)\nplt.show()\n\nresiduals = y_train - y_pred_train\nplt.figure(figsize=(8, 4))\nsns.histplot(residuals, kde=True)\nplt.title(\"Residuals Distribution\")\nplt.xlabel(\"Residuals\")\nplt.grid(True)\nplt.show()\n\n# 10. Test Prediction\ntest_preds = xgb_model.predict(X_test)\nsample_submission[\"label\"] = test_preds\nsample_submission.to_csv(\"submission_whisper_xgb.csv\", index=False)\nprint(\"üìÅ Submission saved as submission_whisper_xgb.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Grammar Scoring from Audio using Whisper and XGBoost\nüîç Overview\nThis project aims to predict the grammar quality score of spoken English using audio recordings. The core idea is to extract meaningful features from audio‚Äîboth low-level acoustic characteristics and high-level semantic content‚Äîand use a machine learning model to learn the relationship between these features and the grammar score.\n\nüìÅ Dataset Summary\nThe dataset includes:\n\nA training set with audio file names and corresponding grammar scores.\n\nA test set with only audio file names (no labels).\n\nAudio folders containing the actual recordings.\n\nA submission format file showing how to structure predictions.\n\nEach audio file represents a short clip of a person speaking. The grammar score (target) reflects the quality of spoken grammar, likely rated by human annotators or derived through linguistic analysis.\n\nüéß Feature Engineering\nTo build an effective model, we extracted two types of features from each audio file:\n\n1. MFCC (Mel-Frequency Cepstral Coefficients)\nMFCCs are standard acoustic features used in speech processing. For each audio:\n\nWe calculated the mean, standard deviation, and skewness of 13 MFCC coefficients.\n\nThese capture how energy and frequency components behave across time in the voice.\n\nMFCCs reflect the pronunciation, rhythm, and articulation, indirectly signaling grammar proficiency.\n\n2. Whisper Embeddings (from OpenAI Whisper model)\nWe used a pretrained Whisper model from OpenAI to extract deeper, semantic-level features:\n\nThe Whisper model listens to the audio and produces a numerical representation (embedding) of what it \"understands\" from it.\n\nThese embeddings include information about word usage, sentence structure, clarity, and fluency.\n\nWe averaged the encoder outputs to create a fixed-size vector representing the entire audio's content and style.\n\nCombining MFCC and Whisper features gave us a rich representation of both how something was said and what was said.\n\n‚öôÔ∏è Model Selection: XGBoost Regressor\nWe trained an XGBoost regression model to map the extracted features to grammar scores.\n\nWhy XGBoost?\n\nIt performs well with structured, high-dimensional features.\n\nIt handles non-linear relationships effectively.\n\nIt's fast to train, allows for regularization, and is robust to noise.\n\nThe model was trained on the training data and then evaluated to check how well it fits.\n\nüìä Evaluation Results\nTo measure how good our model was, we used two key metrics:\n\nPearson Correlation\nTells us how well the predicted scores correlate with actual grammar scores.\n\nA value close to 1 means our predictions follow the true trends very well.\n\nRMSE (Root Mean Squared Error)\nShows the average prediction error.\n\nLower values indicate more accurate predictions.\n\nThe model showed very strong performance with high Pearson correlation and low RMSE on training data, indicating that it effectively learned the grammar scoring pattern.\n\nüìà Visual Insights\nPredicted vs Actual Plot\nA scatter plot of predicted vs actual scores showed that most points fell close to the diagonal line, indicating accurate predictions.\n\nResidual Plot\nThe residuals (differences between actual and predicted scores) were normally distributed around zero, which is a good sign. It means the model doesn‚Äôt systematically overestimate or underestimate scores.\n\nüì¶ Final Submission\nThe model was applied to the test set, and the predictions were saved in the required format for Kaggle submission. This step completes the pipeline from raw audio to automated grammar score prediction.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}